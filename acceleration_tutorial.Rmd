---
title: 'HMM in Marine Sciences: incorporation of covariates and model selection' 
author: "Marco Gallegos Herrada and Vinky Wang"
output: 
  bookdown::html_document2:
    number_sections: true
    highlight: tango
    toc: yes
    toc_float: yes
    theme: cosmo
editor_options:
  chunk_output_type: console
---

<!-- To be able to have continuous line numbers -->
```{=html}
<style>
body
  { counter-reset: source-line 0; }
pre.numberSource code
  { counter-reset: none; }
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
```


# Tutorial objectives


The aim of this tutorial is to delve into the process of fitting hidden Markov models (HMMs) to accelerometer data, integrating covariates into transition probabilities, and employing a classic model selection criterion to choose the most suitable model from a set of candidates. For all of this procedures, we will be using the R package `MomentuHMM`, introduced in first part of this workshop series. 

The primary learning objectives in this first tutorial are to:

  - Fit a basic HMM to accelerometer data
  - incorporating and interpreting covariates on behaviour transition probabilities
  - visualizing the Depth time series with decoded states


<!-- The goal of this tutorial is to explore how to fit hidden Markov models to accelerometer data, incorporate covariates into the transition probabilities, and use a classic model selection criteria to select the "best" model out of different candidates. -->

<!-- The goal of this tutorial is to explore fitting hidden Markov models to accelerometer data, incorporate covariates into the transition probabilities, and select for the best candidate model. We use 4 days of acceleration data obtained from a free-ranging blacktip reef shark at Palmyra Atoll in the central Pacific Ocean (data taken from Leos-Barajas et al. 2017). Specificly, the following topics wil be covered throughout this tutorial: -->

  <!-- - Resampling data (Down sampling) (???) -->


# Accelerometer data

Accelerometer devices measure up to three axes, which can be described relative to the body of the animal: longitudinal (surge), lateral (sway) and dorsoventral (heave). These devices are becoming more prevalent in the fields of animal biologging data as they provide a means of measuring activity in a meaningful and quantitative way. From tri-axial acceleration data, we can also derive several measures that summarize effort or exertion and relate acceleration  to activity levels such as overall dynamic body acceleration (ODBA) and vectorial dynamic body acceleration (VeDBA).

```{r,fig.align='center',echo=F}

knitr::include_graphics("ODBA_pic.png")


```

ODBA and VeDBA can be used to reduce the dimensionality of three-dimension acceleration data while retaining important information. Further, because acceleration data is often at high temporal resolutions over time, it also naturally exhibits a large degree of autocorrelation, making it impossible to assume independence between sequential observations. HMMs can account for the autocorrelation present in the data while assuming that the data were generated according to a finite set of (unobserved) behaviors making them a good candidate model for this type of data structure. 

## Blacktip Reef Shark data

In this tutorial, we will be analyzing four days' worth of acceleration data from a free-ranging blacktip reef shark in the Palmyra Atoll located in the central Pacific Ocean. The acceleration data was collected from a 117-cm female shark (*Carcharhinus melanopterus*) species using a multisensor package. This package was attached to the shark's dorsal fin and recorded three-dimensional acceleration data at a rate of 20 Hz. It also recorded depth and water temperature at a rate of 1 Hz. After four days, the package, which was equipped with a VHF transmitter, detached from the shark and could be retrieved from the water surface (Papastamatiou et al. 2015). To assess the shark's active behavior, the authors calculated the average ODBA over 1-second intervals, resulting in a dataset comprising 320,214 observations. Consequently, the variables in the dataset consist of time of day, water temperature (in Celsius), depth (in meters), and ODBA.

<!-- In this tutorial, we will work with four days of acceleration data gathered from a free-ranging blacktip reef shark at Palmyra Atoll in the central Pacific Ocean. Acceleormetry data was obtained from a free-ranging blacktip reef shark (*Carcharhinus melanopterus*) at Palmyra Atoll in the central Pacific Ocean (data taken from Leos-Barajas et al. 2017). A multisensor package was attached to the dorsal fin of a 117-cm female shark. The multisensor data-logger recorded three-dimensional acceleration (at 20 Hz), depth and water temperature (at 1 Hz) and was embedded in a foam float which detached from the shark after 4 days (Papastamatiou et al. 2015). The package also contained a VHF transmitter allowing recovery at the surface -->
<!-- after detachment. In order to examine active behaviour, the average ODBA of the shark over 1-second intervals was calculated. This resulted in 321 815 observations. As a result, the variables for the dataset are the time of day, water temperature, depth (in meters) and the ODBA. -->

```{r}
# R packages that will be used for this tutorial
library(readr)
library(momentuHMM)
library(ggplot2)
library(dplyr)
library(lubridate)


# Load data 
BlacktipB <- read_delim("BlacktipB.txt", 
                        delim = "\t", escape_double = FALSE, 
                        trim_ws = TRUE)

```

```{r}
head(BlacktipB)
```

## Data processing 

Looking at the ODBA values throughout the observed period, we find ODBA is unusually high at some times -- for this shark we assumed that values between 0 and 2 were consistent with what we expected. The dashed red line in the plot below corresponds to a ODBA value of 2, so those values that are above this line correspond to these high ODBA values throught the observed time period:

```{r, fig.height= 4, fig.width=12,echo=F}

BlacktipB_aux = BlacktipB %>% 
  mutate(Time = as.POSIXct(Time,format = "%m/%d/%Y %H:%M")) #%>% 
  # mutate(hour_to_sec =  as.integer(seconds(hm(format(Time, format = "%H:%M"))))) %>%
  # group_by(Time) %>% mutate(sec = row_number()) %>% ungroup() %>% 
  # mutate(sec = case_when(hour_to_sec == 53160 ~ as.integer(sec + 55),
  #                        TRUE ~ sec),
  #        hour_to_sec = hour_to_sec + sec)

BlacktipB_aux %>% 
  ggplot(aes(Time,ODBA)) + 
  geom_line() + geom_hline(yintercept = 2,linetype="dashed",color="red")
```

Because accommodating extreme values can pose a problem for identification of an adequate state-dependent distribution in our HMM, we removed them from the data set. However, note that in general, deciding whether to remove extreme values or not will more likely depend on whether we find appropriate distributional forms that can accommodate them. Generally, we need to make sure that extreme values are in fact some artefact of the data collection process, not representative of a behavior of interest, or inconsistent with what we are trying to capture as well. Removing data is not good general practice but instead we can assess on a case-by-case basis. Aditionally, for the interest of time, **in this tutorial we will be working with the 1-minute average depth, temperature and ODBA values** -- this decrease significantly the amount of time required for the model fitting stage.

```{r}

# Transform into proper time format and take 1-min avg
BlacktipB_1min = BlacktipB %>% filter(ODBA <= 2) %>%  
  mutate(Time = as.POSIXct(Time,format = "%m/%d/%Y %H:%M")) %>% 
  group_by(Time = cut(Time, breaks = "1 min")) %>%
  summarise(ODBA_avg = mean(ODBA),
            temp_avg = mean(Temp),
            depth_avg = mean(Depth)) %>%
  ungroup() %>%
  mutate(Time = as.POSIXct(Time,format = "%Y-%m-%d %H:%M:%S")) %>%
  as.data.frame()
  
head(BlacktipB_1min)
```


We can see the 1-minute average ODBA time series here across the four days: 

```{r,echo=F, fig.height= 4, fig.width=12}
BlacktipB_1min %>% 
  ggplot(aes(Time,ODBA_avg)) + 
  geom_line() 
```

Now, we are interested in finding possible behaviours through the observed process (ODBA). Let's take a quick look at the histogram of the observations.

```{r, echo=F}
hist(BlacktipB_1min$ODBA_avg, 
     breaks = 80, 
     main="Histogram of ODBA", 
     xlab = "ODBA")
```

\textbf{WORK FROM HERE}

This give us an idea that we can be selecting two main behaviours:

After selecting a data resolution and selecting a number of behaviour states, we are ready to start proposing candidate models for this data!

# Model fitting

The best way to start when fitting a hidden Markov model is to moving from simple models to more complex ones. For the purpose of this tutorial, we will be considering 2 behavioural states, with gamma state-dependent distributions -- these two states can be interpreted as proxies for "high-activity" behaviour and "low-activity" behaviour. As well, no additonal data will be incorporated in the state-switching dynamics -- which translates in not including covariances in the transition probabilities allocated in the transition probability matrix. `MomentumHMM` requires we propose initial values in order to perform the model fitting. For the choice of initial parameter values, performing an Exploratory Data Analysis (EDA) can be a good starting point. From the plots above, considering mean values of 0.001 and 0.003 for the high-activity behaviour and low-activity behaviour to be assigned to our state-dependent distributions seems to be reasonable. As part of the data preparation, we need to process the data using the prepData function, which will assign the class `momentuHMMData` to our dataframe. This will allow us to use the functions provided by `momentuHMM`.

```{r}
BlacktipBData = prepData(BlacktipB_1min,coordNames = NULL)
```

Let's fit our model and observe the output. 

```{r, cache=TRUE}
fit1 = fitHMM(BlacktipBData,
              nbStates=2,
              dist=list(ODBA_avg="gamma"),
              Par0 = list(ODBA_avg=c(.001,.003,1,1)))

fit1
```

We can also plot the results to obtain a visual representation of the fitted model.

```{r}
plot(fit1,breaks = 80)
```

Let's look at the pseudo-residuals.

```{r, fig.height= 4, fig.width=12}
plotPR(fit1)
```

We can also compute the most likely sequence of states.

```{r, fig.height= 6, fig.width=12,cache=TRUE}
# identify most likely state using the Viterbi algorithm
BlacktipB_1min$state <- factor(viterbi(fit1))

# proportion of the behaviour states during the observed period
table(BlacktipB_1min$state)/length(BlacktipB_1min$state)

BlacktipB_1min %>% mutate(day = day(Time)) %>%
  ggplot(aes(Time,ODBA_avg)) +
  #facet_wrap(~day,scales = "free_x") +
  geom_line(alpha=.1) +
  geom_point(aes(shape=state,color=state))

```

Let's include retryFits in the fitHMM function. This can take time to run.

```{r, cache=TRUE}
set.seed(147)
fit1_s2 <- fitHMM(BlacktipBData,
                  nbState = 2,
                  dist=list(ODBA_avg="gamma"),
                  Par0 = list(ODBA_avg=c(.001,.003,1,1)),
                  retryFits=10)
fit1_s2
```

Seems nothing changed at all!

Now let's go further and include a high perturbation in one of the initial values (instead of .001 and .003, let's do .001 and 1). Do we still have similar estimated coefficients and log likelihood? (no matter the initial values, the coefficients should be similar)

```{r, cache=TRUE}
fit1_s2_long <- fitHMM(BlacktipBData,
                  nbState = 2,
                  dist=list(ODBA_avg="gamma"),
                  Par0 = list(ODBA_avg=c(.001,1,1,1)))

fit1_s2_long
```

Let's look at the pseudo-residuals.

```{r, fig.height= 4, fig.width=12}
plotPR(fit1_s2_long)
```

You may get warnings.

We can see that there is high autocorrelation and some deviation from normality.

We can also compute the most likely sequence of states. What can we infer from this? Is there something else we can say from this? According to fitted model, can we see if there is any interesting pattern?

```{r, fig.height= 6, fig.width=12}
# identify most likely state using the Viterbi algorithm
BlacktipB_1min$state_wildPar0 <- factor(viterbi(fit1_s2_long))

# proportion of the behaviour states during the observed period
table(BlacktipB_1min$state_wildPar0)/length(BlacktipB_1min$state_wildPar0)

# BlacktipB_1min %>% mutate(day = day(Time)) %>% 
#   ggplot(aes(Time,state_wildPar0)) + facet_wrap(~day) + geom_point()

BlacktipB_1min %>% mutate(day = day(Time)) %>%
  ggplot(aes(Time,ODBA_avg)) +
  #facet_wrap(~day,scales = "free_x") +
  geom_line(alpha=.1) +
  geom_point(aes(shape=state_wildPar0,color=state_wildPar0))


```

Here we can see that there is only one state when we use these new starting values, this is an indication that there may be problems.

# Incorporating covariates

As in Leos-Barajas et al. 2017, we can incorporate other information that may help explain the values of ODBA. In this case, we consider the minute of the day of every observation. Time of day is represented by two trigonometric functions with period 24 h, $cos(2\pi (t/60)/24)$ and $sin(2\pi (t/60)/24$. Using the function cosinor, we can convert our data stream to something that is useful for us. As well, we need to provide the formula corresponding to the regression that will be stored in the transition probability values.

```{r, cache=TRUE}
# formula corresponding to the regression coefficients for the transition probabilities
# re-prep data
BlacktipB_1min <- BlacktipB_1min %>%
  mutate(lag = 0:(n() - 1),
         sin_part = sin(2*pi*lag*(1/60)/24),
         cos_part = cos(2*pi*lag*(1/60)/24)
         )

BlacktipBData = prepData(BlacktipB_1min,coordNames = NULL, covNames = c("sin_part", "cos_part"))

# re-fit model 1
fit1 = fitHMM(BlacktipBData,nbStates=2,dist=list(ODBA_avg="gamma"),Par0 = list(ODBA_avg=c(.1,.3,1,1)))


formula = ~ sin_part + cos_part
Par0_fit2 <- getPar0(model=fit1, 
                     formula=formula)

fit2 = fitHMM(BlacktipBData,nbStates=2,dist=list(ODBA_avg="gamma"),Par0 = Par0_fit2$Par,formula=formula)

fit2
```


```{r, cache=TRUE}
plot(fit2,breaks=80)
```

Let's explore the results. Do the coefficients vary much? What about the ACF? Did the autocorrelation decrease with this innovation?

```{r, fig.height= 4, fig.width=12}
plotPR(fit2)
```

# Model section: Akaike Information Criteria (AIC)

Akaike information criteria (AIC) is a model selection criteria to select models. In a few words, the higher, the better (or equivalently, since values are always negative, the closer to zero, the better).
We can also take a quick look at the Akaike information criteria (AIC) for the two models to do a comparison. 

```{r}
AIC(fit1)
AIC(fit2)
```


# Exercise

Reproduce the analysis from the blacktip shark data, but now consider three beahviour states instead of only two. 

  - Is there any significant difference in ACF compared to the initial 2-state HMM? 
  - Now include the variable `hour_to_sec` as a covariate and refit your extended model. What are the AIC and the log-likelihood values? Based on the Akaike Information Criteria, which seems to be the "best" model? 

**Note:** In the case that you have your own data, feel free to use the code provided for th blacktip shark data analysis and adapt it to analyse your data. Explore!
